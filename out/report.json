{
  "anchor_summary": {
    "topic": "Symbolic Governor as a deterministic and verifiable control component in AI agents",
    "subareas": [
      "AI agent architectures",
      "Symbolic reasoning in AI",
      "Formal verification of software",
      "Deterministic control systems",
      "Integration of symbolic and neural components"
    ],
    "key_terms": [
      "Symbolic Governor",
      "System 2 reasoning",
      "deterministic code",
      "formal verification",
      "LLM (Large Language Model)",
      "agent root of trust"
    ],
    "likely_venues": [
      "AAAI Conference on Artificial Intelligence",
      "International Joint Conference on Artificial Intelligence (IJCAI)",
      "Conference on Neural Information Processing Systems (NeurIPS)",
      "International Conference on Automated Software Engineering (ASE)",
      "Formal Methods in Computer-Aided Design (FMCAD)"
    ],
    "exclusions": [
      "purely neural network based approaches without symbolic components",
      "unverified heuristic methods",
      "non-deterministic or probabilistic agent control systems"
    ]
  },
  "existing_citations_count": 0,
  "seed_expansion_used": false,
  "bib_path": "cases/references.bib",
  "existing_entries_count": 44,
  "new_entries_added_count": 14,
  "new_bibkeys_added": [
    "UnknownSummary",
    "UnknownAI",
    "UnknownUsing",
    "UnknownApproaches",
    "UnknownThe",
    "Webb_2025",
    "UnknownAgentica",
    "UnknownPDFc",
    "UnknownRoot",
    "UnknownRoota",
    "UnknownAIa",
    "2023",
    "UnknownThea",
    "UnknownDeterministica"
  ],
  "warnings": [],
  "claims": [
    {
      "sid": "S0",
      "sentence": "The Symbolic Governor acts as the \"System 2\" component of the agent.",
      "rationale": "The sentence defines the role of the Symbolic Governor as the 'System 2' component, which is a specific conceptual claim requiring citation to prior work or authoritative sources.",
      "claim_type": "definition",
      "queries": [
        "Symbolic Governor System 2 reasoning deterministic control AI agents",
        "formal verification Symbolic Governor AI agent architectures",
        "deterministic code Symbolic Governor agent root of trust",
        "integration symbolic and neural components Symbolic Governor",
        "Symbolic Governor formal methods deterministic AI control",
        "System 2 reasoning Symbolic Governor LLM formal verification"
      ],
      "selected": [
        {
          "paper_id": null,
          "title": "A Governance-First Paradigm for Principled Agent Engineering - arXiv",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "- \u2022\n\n  A Formal Architecture to Build With: We propose a neuro-symbolic architecture centered on a deterministic Symbolic Governor, which acts as the system\u2019s trusted OS kernel. The Governor\u2019s role is to provide fine-grained, intra-agent governance\u2014imposing rules and safety checks directly on the agent\u2019s workflow.... This distinction is not philosophical; it is a fundamental principle of robust system design: the governor must be separate from the governed. Expecting a model to enforce these systemic rules on itself is a category error. It is akin to allowing a brilliant lawyer to write a contract and also serve as the sole judge and jury in any dispute over it. The model can be the brilliant lawyer that drafts the agent\u2019s logic, but it cannot be its own final, deterministic arbiter. This demonstrates the enduring and non-negotiable need for an external, architectural governance layer like ArbiterOS to safely manage an agent\u2019s interaction with the world.... - \u2022\n\n  For today\u2019s knowledge-centric agents, the Symbolic Governor provides the deterministic, architectural guarantees needed to manage the inherent unreliability of the Probabilistic CPU, solving the immediate reliability gap.\n- \u2022\n\n  For tomorrow\u2019s experience-centric agents, this same architecture provides the essential safety framework. The Symbolic Governor acts as a trusted, auditable supervisor, allowing a learning agent to safely explore, adapt, and self-modify within the bounds of human-defined constitutional rules.... This architecture achieves its goal by separating the agentic system into two distinct components (see Fig.\u00a02). This division of labor is analogous to the \u201cSystem 1\u201d (fast, intuitive) and \u201cSystem 2\u201d (slow, deliberate) models of cognition (Kahneman, 2011). While LLMs can be prompted to exhibit both types of reasoning (Li et\u00a0al., 2025), the foundational split in ArbiterOS is more architecturally precise: it is the immutable separation between the untrusted, probabilistic reasoning engine and the deterministic, trusted governor that controls the reasoning process.... - \u2022\n\n  The Symbolic Governor (\u201cSystem 2\u201d): In direct contrast, this is the agent\u2019s deterministic, auditable \u2018symbolic\u2019 component\u2014the ArbiterOS Kernel. It is a rule-based engine responsible for the high-level orchestration of the agent, the strict enforcement of declarative policies, and making discrete, verifiable decisions. It acts as the system\u2019s arbiter of trust, governing the Probabilistic CPU through a formal instruction set.",
          "doi": null,
          "url": "https://arxiv.org/html/2510.13857v1",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.4,
          "support": 0.3,
          "authority": 0.5,
          "final": 0.365,
          "evidence_snippet": "Neuro-symbolic systems combine neural and symbolic computation, with symbolic logic providing interpretable, rule-based control.",
          "why": "This paper explains the neuro-symbolic paradigm and the complementary roles of neural and symbolic components, supporting the idea of a symbolic 'System 2' but without naming the Symbolic Governor."
        },
        {
          "paper_id": null,
          "title": "Deterministic Agent Controllers: Foundations, Applications, and ...",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "Deterministic agent controllers form a critical class of control systems characterized by their predictable and reproducible behavior. These controllers operate based on precisely defined rules and models, ensuring that given the same initial conditions and inputs, the system's future states are uniquely determined . This fundamental characteristic distinguishes them sharply from their non-deterministic or stochastic counterparts.... The design and operation of deterministic agent controllers are anchored in several key principles:\n\nThe primary differentiation lies in the nature of system evolution and the type of guarantees that can be made.... Deterministic agent controllers are typically built from modular components interacting within defined architectures. These architectures dictate how agents perceive, reason, and act to ensure reliable and predictable operations .\n\nMost AI agents comprise interconnected modular components that process information and execute actions:\n\nA fundamental principle is the\n\n**Perception\u2013Reasoning\u2013Action (PRA) Loop**, where agents continuously observe their environment, decide on actions, execute them, and learn from the outcomes . The predictability of deterministic agents relies on how decisions are made within this loop.... **Decision-Making Mechanisms:**\n\nVarious architectural patterns structure how these components interact, reinforcing deterministic behavior:\n\nReliability in deterministic agent systems is an architectural property, earned through specific design choices that ensure predictable and governed behavior:\n\nDeterministic and stochastic agent controllers represent fundamentally different approaches to system design, each with distinct operational characteristics, performance trade-offs, and suitable application scenarios . The choice between them depends heavily on the problem domain, the level of uncertainty involved, and the desired accuracy and performance . This section elaborates on the operational characteristics and performance of deterministic agent controllers, comparing them with their stochastic counterparts where relevant to highlight their inherent trade-offs.... A deterministic agent controller operates in an environment where the outcome of any action is entirely determined by the current state and action, with no randomness involved . Given initial conditions and actions, the environment will always produce the same outcome . Deterministic models are based on precise inputs and produce the same output for a given set of inputs .... **Agentic AI Systems:** These systems represent a new frontier, integrating autonomy and decision-making by continuously perceiving, reasoning, acting, and learning without explicit human intervention . The next evolutionary step is their transition from digital ecosystems to physical environments, demanding advancements in sensory integration, robotics, and real-time decision-making, where deterministic controls will be vital for safe operation . The process of agentic AI\u2014perceiving, reasoning using large language models (LLMs), acting through APIs, and continuous learning\u2014can be structured with deterministic components to ensure predictable and safe execution .",
          "doi": null,
          "url": "https://mgx.dev/insights/deterministic-agent-controllers-foundations-applications-and-future-trends/7bb058823a4a4b1184fbaddf8555b124",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.4,
          "support": 0.3,
          "authority": 0.5,
          "final": 0.365,
          "evidence_snippet": "Neuro-symbolic systems combine neural and symbolic computation, with symbolic logic providing interpretable, rule-based control.",
          "why": "This paper explains the neuro-symbolic paradigm and the complementary roles of neural and symbolic components, supporting the idea of a symbolic 'System 2' but without naming the Symbolic Governor."
        },
        {
          "paper_id": null,
          "title": "\ud83e\uddea Summary",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "## \ud83d\udcdc The Symbolic Learning Paper\n\nThe core idea behind the symbolic learning paper is to\n\n**bridge the gap between heuristic program design and machine learning**. Rather than training neural models end-to-end, the authors suggest that agents can be driven by structured, human-readable rules. These symbolic rules:\n\n- Apply at specific stages of reasoning or problem-solving,\n\n- Modify the behavior of agents through declarative attributes (like prompt templates or model configs),\n\n- Can be learned from data by analyzing their outcomes, and\n\n- Support reflection, debugging, and interpretability unlike black-box models.... This turns the learning process into\n\n**symbolic meta-optimization**: instead of just optimizing weights, the system tunes its own strategy space.... ## \ud83e\udde9 Mapping Paper Components to Practical Implementation\n\nThe symbolic learning paper introduces three core components: the\n\n**Symbolic Pattern Learner**, the **Meta-Controller**, and the **Intervention Interface**. Each of these plays a crucial role in enabling self-improvement via rule-driven reasoning. In this section, we\u2019ll map these concepts to their real-world implementation in our agent-based Co AI system and explain the engineering choices that made this integration modular, traceable, and extendable.... ### \ud83e\udded 2. Meta-Controller \u2192\n\n`SymbolicRuleApplier`\n\n**Paper Description**:\n\nApplies learned symbolic rules to guide agent behavior in future runs, effectively acting as a control policy over reasoning stages.\n\n**Our Implementation**:\n\n`SymbolicRuleApplier`dynamically injects rule-based overrides into agent configs (... ### \ud83d\udd01 Why Symbolic Learning Belongs Here\n\nSymbolic learning integrates cleanly into this architecture because:\n\n- It\u2019s\n\n**interpretable**decisions can be logged, traced, and modified.\n\n- It\u2019s\n\n**modular**rules attach to specific agents, prompts, or goals.\n\n- It\u2019s\n\n**learnable**performance feedback lets the system evolve.\n\nBy embedding symbolic learning inside Co AI, we close the loop:\n\n\u2192 define a strategy \u2192 apply it \u2192 evaluate the results \u2192 improve the strategy \u2192\n\nresulting in\n\n**autonomously evolving agent behavior** without retraining models.... **agents**used to reason\n\n- \ud83d\udcdc The\n\n**symbolic rules**that modified behavior\n\n- \u270d\ufe0f The\n\n**prompts**created and modified\n\n- \ud83d\udca1 The\n\n**hypotheses**generated during reasoning\n\n- \ud83d\udcc8 The\n\n**scores**and feedback that measure success\n\n- \ud83d\udd01 The",
          "doi": null,
          "url": "https://aibussin.com/blog/symbolic/",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.4,
          "support": 0.3,
          "authority": 0.5,
          "final": 0.365,
          "evidence_snippet": "Neuro-symbolic systems combine neural and symbolic computation, with symbolic logic providing interpretable, rule-based control.",
          "why": "This paper explains the neuro-symbolic paradigm and the complementary roles of neural and symbolic components, supporting the idea of a symbolic 'System 2' but without naming the Symbolic Governor."
        },
        {
          "paper_id": null,
          "title": "AI Agents as Neuro-Symbolic Systems?",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "{ts:153} go through multiple reasoning steps and part of those reasoning steps is the fact that the agent can also call tools\n{ts:161} so it can get some external information or do something else that's what I'm visualizing here right so we have our\n{ts:167} question uh this is from the react paper which I have linked to um but the example is okay aside from the Apple... {ts:198} what we're doing is okay we have these tools that the LM can use we provide them uh and we're also prompting the\n{ts:205} agent uh the LM sorry to say okay go through these multiple steps of uh reasoning and action so that is where\n{ts:213} the react comes from so it's re and act from action and it goes through these steps... {ts:631} you have this major premise then you have a minor premise and I haven't done this for a long time so forgive me if\n{ts:640} I'm not super accurate um but you have a major premise minor premise and conclusion based on that so the idea is\n{ts:646} like if you say something like um all all dogs have four lapes which is maybe not actually true but let's just... {ts:1328} right so this like political route is just this is more for like a guard rail right so this would be a this would be\n{ts:1333} an actual guard rail here as you know like protection basically and okay that's fine whatever\n{ts:1341} that that is just one example then we have the ask LM route and this is a better example so the ask LM route all... {ts:1493} just the LM it's it's also the embedding here right especially if you're not even you know not even including an LM here\n{ts:1501} right there's like to me this system without an LM is pretty agentic to me right that\n{ts:1512} seems to me to be an agent um and then even more so when you add the LM and decision making in there... {ts:1567} now LM is making this decision that's fine no problem right but it goes down these two\n{ts:1573} different paths right so it says tool a or tool B and let's say if we have used tool a for you know whatever that is\n{ts:1580} maybe it's reading about the news um whereas tool B is oh someone's asking a math question",
          "doi": null,
          "url": "https://www.youtube.com/watch?v=JaHfCrVTYF4",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.4,
          "support": 0.3,
          "authority": 0.5,
          "final": 0.365,
          "evidence_snippet": "Neuro-symbolic systems combine neural and symbolic computation, with symbolic logic providing interpretable, rule-based control.",
          "why": "This paper explains the neuro-symbolic paradigm and the complementary roles of neural and symbolic components, supporting the idea of a symbolic 'System 2' but without naming the Symbolic Governor."
        },
        {
          "paper_id": null,
          "title": "Using Symbolic Rules to Steer and Evolve AI - Programmer.ie",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "## \ud83d\udcdc The Symbolic Learning Paper\n\nThe core idea behind the symbolic learning paper is to\n\n**bridge the gap between heuristic program design and machine learning**. Rather than training neural models end-to-end, the authors suggest that agents can be driven by structured, human-readable rules. These symbolic rules:\n\n- Apply at specific stages of reasoning or problem-solving,\n\n- Modify the behavior of agents through declarative attributes (like prompt templates or model configs),\n\n- Can be learned from data by analyzing their outcomes, and\n\n- Support reflection, debugging, and interpretability unlike black-box models.... This turns the learning process into\n\n**symbolic meta-optimization**: instead of just optimizing weights, the system tunes its own strategy space.... ### \ud83e\udded 2. Meta-Controller \u2192\n\n`SymbolicRuleApplier`\n\n**Paper Description**:\n\nApplies learned symbolic rules to guide agent behavior in future runs, effectively acting as a control policy over reasoning stages.\n\n**Our Implementation**:\n\n`SymbolicRuleApplier`dynamically injects rule-based overrides into agent configs (... **Why this design**:\n\n- Agents remain\n\n**stateless and configurable**, making them composable and traceable.\n\n- All interventions are logged via structured rules and linked to evaluations, enabling downstream optimization (e.g.,\n\n`RuleEffectAnalyzer`).... **\ud83e\udde0 Agent Nodes**Modular components like\n\n`ChainOfThoughtAgent`,\n\n`SharpeningAgent`, or\n\n`RetrieverAgent`that execute reasoning stages. Each agent is aware of symbolic rules and can be configured or altered based on them.\n\n\n\n**\ud83e\ude84 Symbolic Rule Applier**This is where symbolic learning shines. Rules are applied just-in-time to guide agent behavior, prompt structure, model choice, or pipeline composition. Example: \u201cUse... ### \ud83d\udd01 Why Symbolic Learning Belongs Here\n\nSymbolic learning integrates cleanly into this architecture because:\n\n- It\u2019s\n\n**interpretable**decisions can be logged, traced, and modified.\n\n- It\u2019s\n\n**modular**rules attach to specific agents, prompts, or goals.\n\n- It\u2019s\n\n**learnable**performance feedback lets the system evolve.\n\nBy embedding symbolic learning inside Co AI, we close the loop:\n\n\u2192 define a strategy \u2192 apply it \u2192 evaluate the results \u2192 improve the strategy \u2192\n\nresulting in\n\n**autonomously evolving agent behavior** without retraining models.... ### \u2699\ufe0f Declarative System Control\n\nBy placing this logic early in each pipeline execution, symbolic rules give us full declarative control over:",
          "doi": null,
          "url": "https://programmer.ie/post/symbolic/",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.4,
          "support": 0.3,
          "authority": 0.5,
          "final": 0.365,
          "evidence_snippet": "Neuro-symbolic systems combine neural and symbolic computation, with symbolic logic providing interpretable, rule-based control.",
          "why": "This paper explains the neuro-symbolic paradigm and the complementary roles of neural and symbolic components, supporting the idea of a symbolic 'System 2' but without naming the Symbolic Governor."
        }
      ],
      "status": "NEED_MANUAL",
      "notes": "No candidates met thresholds. Provide manual review."
    },
    {
      "sid": "S1",
      "sentence": "It is the root of trust.",
      "rationale": "The phrase 'root of trust' is a technical term that requires citation to clarify its specific meaning in this context.",
      "claim_type": "definition",
      "queries": [
        "Symbolic Governor root of trust deterministic control AI agents",
        "formal verification Symbolic Governor System 2 reasoning AI",
        "deterministic code Symbolic Governor agent root of trust",
        "Symbolic Governor integration symbolic neural AI agents",
        "Symbolic Governor formal verification deterministic control systems",
        "Symbolic Governor agent architectures root of trust"
      ],
      "selected": [
        {
          "paper_id": null,
          "title": "Deterministic Agent Controllers: Foundations, Applications, and ...",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "Deterministic agent controllers form a critical class of control systems characterized by their predictable and reproducible behavior. These controllers operate based on precisely defined rules and models, ensuring that given the same initial conditions and inputs, the system's future states are uniquely determined . This fundamental characteristic distinguishes them sharply from their non-deterministic or stochastic counterparts.... The design and operation of deterministic agent controllers are anchored in several key principles:\n\nThe primary differentiation lies in the nature of system evolution and the type of guarantees that can be made.... |Guarantees|\"Certain\" guarantees (e.g., \"reach state T,\" \"remain in set T\") under specified conditions .|\"Probability one\" guarantees (almost certain, but not absolutely certain) .|\n|Examples|Mechanical systems under well-defined forces, classical industrial controllers.|LLM-based agents that operate via high-dimensional stochastic mappings .|... Deterministic agent controllers are typically built from modular components interacting within defined architectures. These architectures dictate how agents perceive, reason, and act to ensure reliable and predictable operations .\n\nMost AI agents comprise interconnected modular components that process information and execute actions:\n\nA fundamental principle is the\n\n**Perception\u2013Reasoning\u2013Action (PRA) Loop**, where agents continuously observe their environment, decide on actions, execute them, and learn from the outcomes . The predictability of deterministic agents relies on how decisions are made within this loop.... **Decision-Making Mechanisms:**\n\nVarious architectural patterns structure how these components interact, reinforcing deterministic behavior:\n\nReliability in deterministic agent systems is an architectural property, earned through specific design choices that ensure predictable and governed behavior:\n\nDeterministic and stochastic agent controllers represent fundamentally different approaches to system design, each with distinct operational characteristics, performance trade-offs, and suitable application scenarios . The choice between them depends heavily on the problem domain, the level of uncertainty involved, and the desired accuracy and performance . This section elaborates on the operational characteristics and performance of deterministic agent controllers, comparing them with their stochastic counterparts where relevant to highlight their inherent trade-offs.... A deterministic agent controller operates in an environment where the outcome of any action is entirely determined by the current state and action, with no randomness involved . Given initial conditions and actions, the environment will always produce the same outcome . Deterministic models are based on precise inputs and produce the same output for a given set of inputs .... The field of deterministic agent controllers, particularly in its hybrid forms, continues to evolve rapidly. Several future directions and open problems are anticipated to guide ongoing research:",
          "doi": null,
          "url": "https://mgx.dev/insights/deterministic-agent-controllers-foundations-applications-and-future-trends/7bb058823a4a4b1184fbaddf8555b124",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.7,
          "support": 0.6,
          "authority": 0.6,
          "final": 0.6349999999999999,
          "evidence_snippet": "What many organizations are just beginning to describe in theory, CFE has already implemented in real, operational form ... DID = the AI\u2019s identity card ... the root of the agent\u2019s identity.",
          "why": "Discusses 'meaning root' and identity roots in AI trust layers, which is related but more abstract than the classical root of trust concept."
        },
        {
          "paper_id": null,
          "title": "[PDF] Provable Determinism for Software in Cyber-Physical Systems",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "tation. In this paper, we present the first formal operational semantics for\nthe Reactor model, and prove its key properties of progress and determin-\nism. The Reactor model and its associated proofs are fully mechanized\nin the Lean theorem prover. As an operational model, our semantics are\nclose to the intuition for implementation and a helpful reference. The... for defining the semantics of programming languages [45,46,57]. We define small-\nstep operational semantics for the Reactor model, providing a foundation to the\nprogramming model that is simple and intuitive, yet rigorous.\nWith these operational semantics, we prove two key properties of the model:\nprogress (execution does not get \u201cstuck\u201d) and determinism. The model and its... associated proofs are fully mechanized6 in the Lean theorem prover [41]. This\nmechanization provides several advantages. First, it forces us to be precise about\notherwise implicit definitions and assumptions, which is especially important in\nthe context of verifying computation [28]. In particular, we find multiple ways in\nwhich the denotational semantics in [36,35] are imprecise, and make them precise.... is, reactors can be nested in other reactors and thus form a tree structure. In\nfact, the entire model is represented by the single Helicopter reactor which\ncontains all other reactors. The Timer reactor is used to implement polling by\nsetting a value on its signal port every 33 milliseconds, which in turn triggers... into multiple type classes, as different parts of the formalization require varying\ndegrees of constraints.\nAt the base of this type class hierarchy is the class of (basic) Reactors. It states\nthat a reactor contains the following identifiable components: input ports, output\nports, state variables, actions, reactions and nested reactors. By \u201cidentifiable\u201d... the function application cptType(cpt, \u03b1). The type Option(A) extends a type A\nby the distinguished element none. Thus, we consider get? to be a partial function\n(a function only defined on a subset of its inputs) by returning none when the\nfunction is undefined for a given input. The Component type defines labels for... sociated type being Value. Thus, we collectively call them ValuedComponents.\nThe Value type is an opaque type (in the same way as ID) of values with a dis-\ntinguished absent element. Reactions, on the other hand, as the smallest units\nof computation in the Reactor model, are essentially functions with additional",
          "doi": null,
          "url": "https://goens.org/publications/rossel-vstte-23/vstte23.pdf",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.7,
          "support": 0.6,
          "authority": 0.6,
          "final": 0.6349999999999999,
          "evidence_snippet": "What many organizations are just beginning to describe in theory, CFE has already implemented in real, operational form ... DID = the AI\u2019s identity card ... the root of the agent\u2019s identity.",
          "why": "Discusses 'meaning root' and identity roots in AI trust layers, which is related but more abstract than the classical root of trust concept."
        },
        {
          "paper_id": null,
          "title": "Root-of-Trust Abstractions for Symbolic Analysis",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "authorization values or policies, and to maintain state between subsequent com-\nmands. To load or use a TPM object a session must be created, and the user will\nindicate what assertions must be checked. The TPM checks the assertions and\nupdates the session attribute policyDigest (a hash chain) if they succeed. If... policyDigest matches the authPolicy for a given object, then access to that\nobject is granted. We refer the reader to [24, \u00a719.7] for the complete details. For\nillustration purposes, we provide an example in Appendix A.\n2.2\nRemote Attestation\nRemote attestation [21,14] is a mechanism to provide evidence of the integrity... to be certified by a third party called the Privacy Certification Authority (PCA).\nThe certification process, detailed in Sec. 4, implies that the PCA knows the\nrelationship between EK and AKs, but the PCA is trusted not to reveal this\ninformation, which would break the anonymity of the platform. A verifier can... trust the platform if it successfully verifies that a quote is a valid signature over\nexpected PCR values with an AK certified by a PCA.\nWe also note that the TCG has an alternative method for performing remote\nattestation without revealing the EK to a trusted third party, which is known as\nthe Direct Anonymous Attestation (DAA) [5]. However, DAA works by design... keys, respectively. We denote certA(x) a certificate for object x issued by entity\nA, and signA(y) a signature of y using private key Apriv. Also, for a TPM key\nk we denote kpub and kpriv its public and private key parts, kname its name,\nkauthPolicy its EA policy, and kh its TPM object handle [24, \u00a715]. For clarity... creates an AK using the TPM and this AK is certified by the PCA. (2) the\nClient creates a TLS key using the TPM, which is certified by the PCA and\nsigned by the SP. (3) Finally, the Client uses the TLS key to establish a secure\ncommunication channel with the SP, for exchanging encrypted messages and... to quote the PCR referenced in pcrSelection, where quote = H(pcrDigest \u2225\nqData) and signature = signakpriv(quote). The pair (quote, signature) and the\nAK certificate are sent to the SP, who verifies the signature and the PCR values\nthat reflect the trust status of the platform. See Fig. 1(c).",
          "doi": null,
          "url": "https://ubitech.eu/wp-content/uploads/Root-of-Trust-Abstractions-for-Symbolic-Analysis-Application-to-Attestation-Protocols.pdf",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.7,
          "support": 0.6,
          "authority": 0.6,
          "final": 0.6349999999999999,
          "evidence_snippet": "What many organizations are just beginning to describe in theory, CFE has already implemented in real, operational form ... DID = the AI\u2019s identity card ... the root of the agent\u2019s identity.",
          "why": "Discusses 'meaning root' and identity roots in AI trust layers, which is related but more abstract than the classical root of trust concept."
        },
        {
          "paper_id": null,
          "title": "University of Birmingham",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "Root-of-trust abstractions for symbolic analysis\nFotiadis, Georgios; Moreira-Sanchez, Jose; Giannetsos, Thanassis; Chen, Liqun; Ronne,\nPeter B.; Ryan, Mark; Ryan, Peter Y.A.\nDOI:\n10.1007/978-3-030-91859-0_9... Root-of-Trust Abstractions for Symbolic\nAnalysis: Application to Attestation Protocols\nGeorgios Fotiadis1, Jos\u00b4e Moreira2, Thanassis Giannetsos3, Liqun Chen4,\nPeter B. R\u00f8nne1, Mark D. Ryan2, and Peter Y.A. Ryan1... and verification of the the Direct Anonymous Attestation (DAA) protocol of\nthe TPM. On the other hand, Delaune et al. [10], propose a Horn-clause frame-\nwork where they prove its soundness and use ProVerif to approximate the TPM\ninternal state space, helping to address non-termination issues.... perspective of formally verifying RoT-based applications, this model represents\na means of reasoning about security and privacy (of offered services) without\nbeing bogged down by the intricacies of various crypto primitives considered\nin the different platforms. We conduct our analysis in the symbolic model of\ncryptography (Dolev-Yao adversary [11]) through the Tamarin prover [4] and its... processor during boot, and starts a chain of measurements. The RoT for storage\nand reporting are responsibilities assigned to the TPM, typically implemented\nas a tamper-resistant hardware embedded in a host device. The TPM and the\nhost device form a platform.\nA measurement chain of trust is a mechanism that allows to establish trust... authorization values or policies, and to maintain state between subsequent com-\nmands. To load or use a TPM object a session must be created, and the user will\nindicate what assertions must be checked. The TPM checks the assertions and\nupdates the session attribute policyDigest (a hash chain) if they succeed. If... policyDigest matches the authPolicy for a given object, then access to that\nobject is granted. We refer the reader to [24, \u00a719.7] for the complete details. For\nillustration purposes, we provide an example in Appendix A.\n2.2\nRemote Attestation\nRemote attestation [21,14] is a mechanism to provide evidence of the integrity",
          "doi": "10.1038/165303c0",
          "url": "https://pure-oai.bham.ac.uk/ws/portalfiles/portal/159106790/ESORICS_2021_paper.pdf",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.7,
          "support": 0.6,
          "authority": 0.6,
          "final": 0.6349999999999999,
          "evidence_snippet": "What many organizations are just beginning to describe in theory, CFE has already implemented in real, operational form ... DID = the AI\u2019s identity card ... the root of the agent\u2019s identity.",
          "why": "Discusses 'meaning root' and identity roots in AI trust layers, which is related but more abstract than the classical root of trust concept."
        },
        {
          "paper_id": null,
          "title": "Root-of-Trust Abstractions for Symbolic",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "state [2,7]. Some notable examples in the context of the TPM are the works\nby Shao et al., which cover specific subsets of TPM functionalities, such as En-\nhanced Authorization (EA) [23] or HMAC authorization [22], identifying misuse\ncases. Also, Xi et al. [30] and Wesemeyer et al. [29] conduct formal analysis... and verification of the the Direct Anonymous Attestation (DAA) protocol of\nthe TPM. On the other hand, Delaune et al. [10], propose a Horn-clause frame-\nwork where they prove its soundness and use ProVerif to approximate the TPM\ninternal state space, helping to address non-termination issues.... authorization values or policies, and to maintain state between subsequent com-\nmands. To load or use a TPM object a session must be created, and the user will\nindicate what assertions must be checked. The TPM checks the assertions and\nupdates the session attribute policyDigest (a hash chain) if they succeed. If... policyDigest matches the authPolicy for a given object, then access to that\nobject is granted. We refer the reader to [24, \u00a719.7] for the complete details. For\nillustration purposes, we provide an example in Appendix A.\n2.2\nRemote Attestation\nRemote attestation [21,14] is a mechanism to provide evidence of the integrity... user first creates an AK that will be used to sign attestation reports (quotes).\nA quote is essentially composed of the contents stored in selected PCRs (which\nreflect the platform state) signed with with the AK. As commented above, the\nuser has the ability to create as many AKs as they wish, but each AK is required... to be certified by a third party called the Privacy Certification Authority (PCA).\nThe certification process, detailed in Sec. 4, implies that the PCA knows the\nrelationship between EK and AKs, but the PCA is trusted not to reveal this\ninformation, which would break the anonymity of the platform. A verifier can... trust the platform if it successfully verifies that a quote is a valid signature over\nexpected PCR values with an AK certified by a PCA.\nWe also note that the TCG has an alternative method for performing remote\nattestation without revealing the EK to a trusted third party, which is known as\nthe Direct Anonymous Attestation (DAA) [5]. However, DAA works by design",
          "doi": null,
          "url": "https://markryan.eu/research/papers/pdf/21-esorics.pdf",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.7,
          "support": 0.6,
          "authority": 0.6,
          "final": 0.6349999999999999,
          "evidence_snippet": "What many organizations are just beginning to describe in theory, CFE has already implemented in real, operational form ... DID = the AI\u2019s identity card ... the root of the agent\u2019s identity.",
          "why": "Discusses 'meaning root' and identity roots in AI trust layers, which is related but more abstract than the classical root of trust concept."
        }
      ],
      "status": "NEED_MANUAL",
      "notes": "No candidates met thresholds. Provide manual review."
    },
    {
      "sid": "S2",
      "sentence": "Unlike the LLM, the Governor is implemented in deterministic code (e.",
      "rationale": "The sentence compares the implementation of the Governor to the LLM, stating it is deterministic code, which is a factual claim requiring citation.",
      "claim_type": "comparison",
      "queries": [
        "Symbolic Governor deterministic code formal verification",
        "Symbolic Governor System 2 reasoning deterministic AI agent",
        "deterministic control systems Symbolic Governor AI agents",
        "formal verification Symbolic Governor agent root of trust",
        "integration of symbolic and neural components deterministic control",
        "Symbolic Governor versus LLM deterministic implementation"
      ],
      "selected": [
        {
          "paper_id": null,
          "title": "A Governance-First Paradigm for Principled Agent Engineering - arXiv",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "This paper posits that this crisis stems from a deeper, fundamental paradigm mismatch: *attempting to command an inherently probabilistic processor using the deterministic mental models of traditional software engineering*. The LLM is a quintessential \u201cSoftware 2.0\u201d component (Karpathy, 2017); its behavior is learned from data rather than being explicitly programmed, a paradigm shift with profound implications for traditional software engineering practices (Dig et\u00a0al., 2021). The artisanal approach of a \u201cprompt wizard\u201d is a brittle attempt to force deterministic behavior from this probabilistic system, treating it like traditional code and failing to manage its inherent uncertainty.... - \u2022\n\n  A Formal Architecture to Build With: We propose a neuro-symbolic architecture centered on a deterministic Symbolic Governor, which acts as the system\u2019s trusted OS kernel. The Governor\u2019s role is to provide fine-grained, intra-agent governance\u2014imposing rules and safety checks directly on the agent\u2019s workflow.... - \u2022\n\n  For today\u2019s knowledge-centric agents, the Symbolic Governor provides the deterministic, architectural guarantees needed to manage the inherent unreliability of the Probabilistic CPU, solving the immediate reliability gap.\n- \u2022\n\n  For tomorrow\u2019s experience-centric agents, this same architecture provides the essential safety framework. The Symbolic Governor acts as a trusted, auditable supervisor, allowing a learning agent to safely explore, adapt, and self-modify within the bounds of human-defined constitutional rules.... This architecture achieves its goal by separating the agentic system into two distinct components (see Fig.\u00a02). This division of labor is analogous to the \u201cSystem 1\u201d (fast, intuitive) and \u201cSystem 2\u201d (slow, deliberate) models of cognition (Kahneman, 2011). While LLMs can be prompted to exhibit both types of reasoning (Li et\u00a0al., 2025), the foundational split in ArbiterOS is more architecturally precise: it is the immutable separation between the untrusted, probabilistic reasoning engine and the deterministic, trusted governor that controls the reasoning process.... - \u2022\n\n  The Symbolic Governor (\u201cSystem 2\u201d): In direct contrast, this is the agent\u2019s deterministic, auditable \u2018symbolic\u2019 component\u2014the ArbiterOS Kernel. It is a rule-based engine responsible for the high-level orchestration of the agent, the strict enforcement of declarative policies, and making discrete, verifiable decisions. It acts as the system\u2019s arbiter of trust, governing the Probabilistic CPU through a formal instruction set.",
          "doi": null,
          "url": "https://arxiv.org/html/2510.13857v1",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.5,
          "support": 0.4,
          "authority": 0.5,
          "final": 0.45,
          "evidence_snippet": "We explore methods for improving LLM steerability and decision-making reliability... through the use of mechanisms to guarantee structured outputs from LLM... structured outputs can be used for executing Program Control Flow.",
          "why": "This paper discusses improving reliability and control over LLMs via structured outputs, hinting at deterministic control but does not explicitly mention a deterministic Governor."
        },
        {
          "paper_id": null,
          "title": "AI's Missing Layer: Why the Future Might Belong to Symbolic ...",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "In this model, the LLM writes rules in a formal language like Lean or Coq, and an independent theorem prover checks correctness. The LLM proposes; the prover disposes. AlphaGeometry is the first major demonstration of this loop.\n\nThis shifts correctness from something humans\n\n*inspect* to something compilers *prove*.... ## 4. QAV as a practical example of symbolic knowledge becoming executable\n\nQAV \u2014 a rules-based value investing methodology \u2014 shows exactly why symbolic systems matter.\n\nToday, QAV\u2019s rules live as prose in a long document. An LLM can interpret the rules, but it can also:\n\n- misapply a threshold\n\n- forget an exception\n\n- blend two rules\n\n- invent a detail... A symbolic QAV engine transforms the method into something deterministic:\n\n- every rule becomes explicit and checkable\n\n- intrinsic value calculations become code, not text\n\n- liquidity and sentiment filters execute exactly\n\n- every recommendation produces a rule trace\n\nThe QAV document remains the human-readable specification.\n\nThe symbolic engine becomes the executable truth.\n\nThe LLM becomes the interface and the maintenance assistant.\n\nThis pattern generalises across thousands of domains.... ## 5. Personal vs centralised symbolic knowledge engines\n\nSome symbolic engines will be personal \u2014 reflecting individual expertise, company-specific processes, research frameworks, or bespoke methodologies.\n\nMany more will be institutional:\n\n- tax ontologies\n\n- medical guidelines\n\n- safety standards\n\n- engineering codes\n\n- regulatory rulebooks\n\nThese will be maintained by governments, standards bodies, and major firms.\n\n**Whichever institutions control the canonical symbolic rulebases will control the reasoning layer of civilisation.**\n\nThis is the real battleground of AI governance in the next decade.... When a latent reasoning model makes a serious error, there is:\n\n- no explicit rule trace\n\n- no clear ontology\n\n- no guarantee the same reasoning holds tomorrow\n\nFor medicine, aviation, critical infrastructure, regulated finance, and national security, that is not acceptable.\n\nIn those domains, society will demand:\n\n- explicit rules and constraints\n\n- provenance of changes\n\n- audit trails\n\n- independent verification\n\nThe argument here is not that symbolic systems win everywhere.\n\nIt\u2019s that\n\n**for the decisions we most care about, opacity is itself a failure mode.**\n\nBoth paradigms will coexist.",
          "doi": null,
          "url": "https://intelletto.com.au/ais-missing-layer-why-the-future-might-belong-to-symbolic-knowledge-engines-connected-by-llms/",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.5,
          "support": 0.4,
          "authority": 0.5,
          "final": 0.45,
          "evidence_snippet": "We explore methods for improving LLM steerability and decision-making reliability... through the use of mechanisms to guarantee structured outputs from LLM... structured outputs can be used for executing Program Control Flow.",
          "why": "This paper discusses improving reliability and control over LLMs via structured outputs, hinting at deterministic control but does not explicitly mention a deterministic Governor."
        },
        {
          "paper_id": null,
          "title": "Hybrid Intelligence",
          "authors": [],
          "year": 2023,
          "venue": null,
          "abstract": "*Balancing Rule-Driven Precision with Generative Flexibility*\n### Executive Summary\nTraditional software, built solely on deterministic, rule-based logic, guarantees reliability and predictability but often struggles to adapt. Large language models (LLMs), by contrast, offer flexibility and cognitive capabilities but can be unpredictable. A hybrid approach \u2014 combining deterministic code with LLM-driven reasoning \u2014 yields systems that are both robust and adaptive. By enforcing hard constraints in code and delegating nuanced, unstructured tasks to an LLM, organizations gain faster innovation cycles without sacrificing transparency, compliance, or control. At New Math Data, we work at the intersection of deterministic software engineering and large language model innovation, building systems that are both reliable and adaptive.... ### Value Proposition\nBy integrating LLM-driven reasoning into deterministic architectures, CTOs can enable software that dynamically responds to unstructured inputs and evolving scenarios (thanks to LLM flexibility) without sacrificing the consistency, security, and auditability mandated by enterprise standards (ensured by rule-based code). This translates into tangible business value: systems that adapt quickly to new data and user needs, reduced development time for complex features, improved decision support with AI insights \u2014 all while mitigating the risks of unpredictable AI behavior via deterministic guardrails.... 3. Limited Understanding of Context. Rule\u2010based systems cannot interpret nuances, synonyms, or evolving data patterns.\n\n4. No Learning or Generalization. They cannot improve autonomously based on new information.\n\nIn summary, traditional logic-driven development remains indispensable for its determinism and reliability, especially for core transaction processing, safety-critical controls, and ensuring compliance. However, its inflexibility and inability to deal with ambiguity or learn limit its effectiveness for many modern applications. This sets the stage for incorporating more intelligent, learning-driven components into our software\u2026 which is where LLMs come in.... **3. Human-in-the-Loop**\n\n\n\n**What:**Automatically escalate low-confidence or high-risk cases to human experts for approval. **Why:**Catches hallucinations and edge-case errors before they reach production, while feeding back corrections to improve the model.\n\n\n\n**4. Explainability & Traceability**\n\n\n\n**What:**Require LLMs to output a brief rationale or confidence score and log both that and any rule-based verdicts. **Why:**Creates an auditable trail \u2014 combining AI \u201cchain-of-thought\u201d with explicit rule labels \u2014 for debugging, compliance, and user trust.",
          "doi": "10.1007/978-981-19-8637-6",
          "url": "https://newmathdata.com/blog/hybrid-ai-deterministic-code-llm-reasoning-systems/",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.5,
          "support": 0.4,
          "authority": 0.5,
          "final": 0.45,
          "evidence_snippet": "We explore methods for improving LLM steerability and decision-making reliability... through the use of mechanisms to guarantee structured outputs from LLM... structured outputs can be used for executing Program Control Flow.",
          "why": "This paper discusses improving reliability and control over LLMs via structured outputs, hinting at deterministic control but does not explicitly mention a deterministic Governor."
        },
        {
          "paper_id": null,
          "title": "The Synergy of Symbolic and Connectionist AI in LLM ...",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "- \u2022\n\nConvergence of Paradigms: This article highlights the convergence of symbolic and connectionist approaches in developing LAAs, emphasizing their enhanced reasoning, decision-making, and efficiency. By contrasting LAAs with Knowledge Graphs (KGs) within neuro-symbolic AI, we examine distinct patterns and functionalities. While both integrate symbolic and neural methodologies, LAAs demonstrate unique advantages over KGs: (1) analogizing human reasoning with agentic workflows and various prompting techniques [12, 13], (2) scaling effectively on large datasets, adapting to in-context samples, and leveraging the emergent abilities of LLMs.... ### 3.3 Rethink LAAs from the Perspective of Neuro-symbolic AI\n\nNeuro-symbolic AI combines the strengths of neural networks and symbolic reasoning, producing decision-making processes that are both explicit and interpretable. In autonomous agents enhanced by LLMs, the latest advancements in deep neural networks are harnessed, while task decomposition and planning are guided by symbolic AI principles \u2014 breaking complex tasks into discrete, logical steps that can be systematically analyzed and reasoned through [69]. This fusion of symbolic structures and deep neural networks creates a powerful synergy, significantly boosting the capabilities of these agents.... This approach mimics the *case-based reasoning*, a fundamental concept in symbolic AI, by leveraging explicit knowledge and experiences to tackle new problems. This enhances the model\u2019s ability to generalize from specific examples, effectively creating a neuro-symbolic mapping from presented examples to desired outcomes.... By prompting large language models with instructions like \u201clet\u2019s think step by step\u201d, these models analogise human\u2019s reasoning processes and can exhibit logical and mathematical reasoning, thereby enhancing their structured reasoning skills [12, 13]. This agentic approach allows LLMs to not only process but also proactively generate structured, logical, and adaptive reasoning pathways [56], significantly improving their problem-solving and decision-making capabilities, marking a pivotal evolution in neuro-symbolic AI technologies.... ## 4 Discussions and Future Directions\n\nIn this section, we discuss the LLM-empowered autonomous agent by comparing it with an alternative neuro-symbolic approach\u2014the Knowledge Graph\u2014and then highlight future directions for this technology.... KGs harness the power of symbolic AI, organizing domain-specific knowledge through explicit relationships and rules. This design makes them highly effective in static environments where precision, interpretability, and predefined schemas are crucial. Their logical reasoning capabilities ensure that outputs are consistent and verifiable, which is paramount for applications needing clarity and exactitude in knowledge modeling\u00a0[74].",
          "doi": null,
          "url": "https://arxiv.org/html/2407.08516v1",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.5,
          "support": 0.4,
          "authority": 0.5,
          "final": 0.45,
          "evidence_snippet": "We explore methods for improving LLM steerability and decision-making reliability... through the use of mechanisms to guarantee structured outputs from LLM... structured outputs can be used for executing Program Control Flow.",
          "why": "This paper discusses improving reliability and control over LLMs via structured outputs, hinting at deterministic control but does not explicitly mention a deterministic Governor."
        },
        {
          "paper_id": null,
          "title": "Deterministic Generation: Skills Make LLMs Actually Reliable",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "## The Problem I Keep Seeing\n\nHaving architected high-availability systems for defense, healthcare, finance, and large-scale gaming, I\u2019ve learned that 24/7 operations leave no room for \u2018the AI might have chosen correctly.\u2019 At 3 a.m., you need deterministic, predictable execution.\n\nLLMs are powerful, but they're fundamentally probabilistic (i.e., LLMs sample outputs rather than guaranteeing the same answer). That's fine for creative work; it's a feature. But when you need the model to call external tools, pass structured arguments, or integrate into production workflows? That non-determinism becomes your enemy.... In practice this means treating each LLM-invoked tool call as a subroutine in an operational system, rather than as free-form text generation. The prompt provides a rigid contract; it is never ambiguous. The schema defines the shape of the call, the validation layer ensures the contract is met, and the execution layer either proceeds or gracefully fails back. By decomposing intelligence into predictable, verifiable skills rather than monolithic reasoning sessions, you shift from \u2018creative AI assistant\u2019 to \u2018operational execution engine\u2019.\u201d... The key insight: the LLM executes skills that are deterministic and lightweight, not context-heavy operations. You're not asking the model to reason deeply about every tool call, instead you're asking it to match patterns and apply structured templates. Heavy context and reasoning happen in the prompt design and validation layers, not in the skill execution itself.... The implementation varies by provider (Claude Skills, OpenAI function calling, whatever), but the principles stay consistent:\n\n**Precise tool definitions**- Machine-readable schemas, not documentation **Structured prompts**- Explicit rules and examples, not vague guidelines **Runtime validation**- Schema checks plus semantic verification **Controlled execution**- Clear fallbacks when things go wrong\n\nThink of it as defensive programming for AI systems.... ## The Trade-offs\n\nOver-constraining can hurt. If you lock down every parameter, you lose flexibility for ambiguous user intents. I've found the sweet spot is: constrain what matters for safety and correctness, but leave room for the model to handle natural language variation.\n\nOne subtle cost: by constraining the model you may suppress emergent use-cases or unexpected insights. Therefore it\u2019s prudent to separate modules of your system: use an unconstrained LLM for discovery or ideation, and a locked-down skill engine for execution. Over time the ideation outputs can feed into new deterministic skills if the use-case matures.",
          "doi": null,
          "url": "https://markrox.dev/AI%20Strategy/2025/11/14/deterministic-generation/",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.5,
          "support": 0.4,
          "authority": 0.5,
          "final": 0.45,
          "evidence_snippet": "We explore methods for improving LLM steerability and decision-making reliability... through the use of mechanisms to guarantee structured outputs from LLM... structured outputs can be used for executing Program Control Flow.",
          "why": "This paper discusses improving reliability and control over LLMs via structured outputs, hinting at deterministic control but does not explicitly mention a deterministic Governor."
        }
      ],
      "status": "NEED_MANUAL",
      "notes": "No candidates met thresholds. Provide manual review."
    },
    {
      "sid": "S3",
      "sentence": "g., Python, Rust) and is formally verifiable.",
      "rationale": "The statement claims formal verifiability of code in specific languages, which requires evidence or citation.",
      "claim_type": "method_description",
      "queries": [
        "Symbolic Governor deterministic control formal verification",
        "Symbolic Governor AI agent System 2 reasoning deterministic code",
        "formal verification deterministic control systems Symbolic Governor",
        "integration symbolic and neural components Symbolic Governor",
        "agent root of trust Symbolic Governor formal verification",
        "Symbolic Governor LLM deterministic AI agent architectures"
      ],
      "selected": [
        {
          "paper_id": null,
          "title": "A Governance-First Paradigm for Principled Agent Engineering - arXiv",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "This paradigm provides control by introducing the Agentic Computer, a mental model that reframes the LLM as a \u201cProbabilistic CPU.\u201d A neuro-symbolic Hardware Abstraction Layer (HAL) that decouples durable agent logic from the underlying model manages this volatile hardware, which is governed by the formal Agent Constitution Framework (ACF) instruction set and continuously verified by the rigorous Evaluation-Driven Development Lifecycle (EDLC).... - \u2022\n\n  A Formal Architecture to Build With: We propose a neuro-symbolic architecture centered on a deterministic Symbolic Governor, which acts as the system\u2019s trusted OS kernel. The Governor\u2019s role is to provide fine-grained, intra-agent governance\u2014imposing rules and safety checks directly on the agent\u2019s workflow.... |Developer-Dependent. Relies on the developer to correctly implement governance logic within the SDK.|Architecturally Enforced. Provides a guaranteed separation of the governor from the governed, making compliance an intrinsic property of the system.|... - \u2022\n\n  For today\u2019s knowledge-centric agents, the Symbolic Governor provides the deterministic, architectural guarantees needed to manage the inherent unreliability of the Probabilistic CPU, solving the immediate reliability gap.\n- \u2022\n\n  For tomorrow\u2019s experience-centric agents, this same architecture provides the essential safety framework. The Symbolic Governor acts as a trusted, auditable supervisor, allowing a learning agent to safely explore, adapt, and self-modify within the bounds of human-defined constitutional rules.... This architecture achieves its goal by separating the agentic system into two distinct components (see Fig.\u00a02). This division of labor is analogous to the \u201cSystem 1\u201d (fast, intuitive) and \u201cSystem 2\u201d (slow, deliberate) models of cognition (Kahneman, 2011). While LLMs can be prompted to exhibit both types of reasoning (Li et\u00a0al., 2025), the foundational split in ArbiterOS is more architecturally precise: it is the immutable separation between the untrusted, probabilistic reasoning engine and the deterministic, trusted governor that controls the reasoning process.... - \u2022\n\n  The Symbolic Governor (\u201cSystem 2\u201d): In direct contrast, this is the agent\u2019s deterministic, auditable \u2018symbolic\u2019 component\u2014the ArbiterOS Kernel. It is a rule-based engine responsible for the high-level orchestration of the agent, the strict enforcement of declarative policies, and making discrete, verifiable decisions. It acts as the system\u2019s arbiter of trust, governing the Probabilistic CPU through a formal instruction set.",
          "doi": null,
          "url": "https://arxiv.org/html/2510.13857v1",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.2,
          "support": 0.1,
          "authority": 0.4,
          "final": 0.18,
          "evidence_snippet": "Formal analysis and verification of TPM functionalities... using ProVerif to approximate internal state space.",
          "why": "This paper is about formal verification in trusted platform modules, unrelated to AI agent architectures or programming languages like Python or Rust."
        },
        {
          "paper_id": null,
          "title": "Approaches to the Formal Verification of",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "and hybrid systems, and specification and verification of hierarchical control\nprograms.\nAddress: see R. Huuck, Tel. +49 431 5604-22, Email: bls@informatik.uni-\nkiel.de\nThis paper presents two different approaches to the\nproblem of formally verifying the correctness of control... lyse a hybrid system. One way to do this is to build for-\nmal models of the system and of the requirements from\nthe specification, and to apply formal methods (i.e., ri-\ngorous analysis in the mathematical or logical sense) to\nprove that the system model has the required proper-\nties. This general procedure is called Formal Verifica-... In Computer Science, two different approaches to for-\nmal verification are distinguished, algorithmic and de-\nductive verification. Algorithmic verification, often cal-\nled model-checking, means that a computer algorithm is\nused which receives a model of a system and a specifica-\ntion of its required behaviour as input, and then checks... an attempt to overcome restrictions which were experi-\nenced for the algorithmic verification by integrating al-\ngorithmic and deductive techniques. The paper is orga-\nnized as follows. In the next section, the work concer-\nned with algorithmic verification is presented. Section\n3 is devoted to the deductive and structured verification... enabled. This means that any finite state trajectory which\nis possible according to the TCES model is extendable to\nan infinite one. For TA, the validity of a state trajectory\nis decided by an acceptance criterion, usually that some\nstates are visited infinitely often. This concept does not\nrequire input-enabledness. It is possible that a model al-... a suite of interfaces, either as front-ends, as e.g. hierar-\nchical block diagram and state graph editors, translation\nfrom control code to TCES, or approximation of swit-\nched continuous CES by TCES, or as back-ends, e.g. in-\nterfaces to TA analysis tools or discrete model-checkers.... nique \u201cTemporal Logic of Actions\u201d (TLA) [20], which\nwas supplemented by a compositional process concept\n(cf. [18]). Processes encapsulate private system state va-\nriables. State transitions are specified by actions each de-\nscribing a class of transitions. As in the formal descripti-",
          "doi": null,
          "url": "https://ls4-www.cs.tu-dortmund.de/RVS/Pub/TS/AT01.pdf",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.7,
          "support": 0.6,
          "authority": 0.3,
          "final": 0.59,
          "evidence_snippet": "This paper presents two different approaches to the problem of formally verifying the correctness of control... formal models of the system and of the requirements from the specification, and to apply formal methods ... to prove that the system model has the required properties.",
          "why": "The abstract discusses formal verification approaches and formal models, which aligns with the claim about formal verifiability, but it does not mention specific programming languages like Python or Rust."
        },
        {
          "paper_id": null,
          "title": "The Synergy of Symbolic and Connectionist AI in LLM-Empowered ...",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "### LAAs: A Converged Approach\n\nThe authors define LLM-empowered Autonomous Agents (LAAs) as systems that blend symbolic reasoning (through predefined rules and knowledge representation) with the generative abilities of LLMs. This synergistic relationship positions LAAs to perform more complex reasoning, planning, and decision-making tasks than traditional AI systems. The core components of LAAs are outlined as follows:... **Neural Subsystem (LLM)**: Acts as the primary engine for processing and generating language. **Symbolic Subsystem**: Manages the integration of defined rules and structured workflows to facilitate reasoning and decision-making. **External Tool Integration**: Provides access to additional capabilities such as databases, APIs, and sensory inputs, enabling a more holistic operational framework.... ### Technical Methods of Integration\n\nThe integration of symbolic and neural methodologies in LAAs is achieved through several mechanisms:... ### Neuro-symbolic Perspective\n\nThe authors argue for framing LAAs within a neuro-symbolic AI framework, suggesting that the fusion of symbolic reasoning with neural network capabilities leads to more interpretable and effective decision-making processes. This involves:\n\n**Symbolic Modeling versus Neural Representation**: Traditional symbolic AI utilizes clear abstractions for reasoning whereas LAAs rely on a more distributed, implicit representation learned from vast text data. **Implicit Reasoning and In-context Learning**: LAAs utilize few-shot in-context learning techniques, allowing them to adapt to new situations without extensive retraining or reconfiguration.... ### Future Directions\n\nThe paper anticipates promising directions such as:\n\n**Neuro-vector-symbolic Integration**: This involves leveraging vector manipulation and high-dimensional representations for enhanced reasoning capabilities and cognitive modeling. **Program-Proof-of-Thoughts (P2oT)**: A proposed methodology that breaks down reasoning tasks into verifiable propositions, utilizing programming languages like Dafny for formal correctness verification.... ### Conclusion\n\nIn sum, the paper underscores the ongoing evolution of AI through the convergence of symbolic and connectionist frameworks, particularly embodied by LLM-empowered Autonomous Agents. The insights provided not only illuminate the transformative potential of these technologies but also propose clear trajectories for future research aimed at further improving neuro-symbolic AI capabilities. The comprehensive analysis of LAAs and their methodologies offers an optimistic outlook on the advancements and applications of AI in a rapidly changing technological landscape.",
          "doi": null,
          "url": "https://www.themoonlight.io/en/review/converging-paradigms-the-synergy-of-symbolic-and-connectionist-ai-in-llm-empowered-autonomous-agents",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.2,
          "support": 0.1,
          "authority": 0.4,
          "final": 0.18,
          "evidence_snippet": "Formal analysis and verification of TPM functionalities... using ProVerif to approximate internal state space.",
          "why": "This paper is about formal verification in trusted platform modules, unrelated to AI agent architectures or programming languages like Python or Rust."
        },
        {
          "paper_id": null,
          "title": "A brain-inspired agentic architecture to improve planning with LLMs",
          "authors": [
            "Webb, Taylor",
            "Mondal, Shanka Subhra",
            "Momennejad, Ida"
          ],
          "year": 2025,
          "venue": null,
          "abstract": "Importantly, we investigate problems in which the planning task (e.g., the set of all possible states S and the transition function *T*) is not formally specified, but is instead informally described in natural language. Rather than receiving the transition function directly, the components of the model must infer from this natural language description which state will result from a particular action, whether a particular action is valid or invalid, etc., meaning that classical planning algorithms cannot be directly applied to these problems.... - **Predictor**. The Predictor receives the current state *s*~*t*~, and a proposed action *a*, and predicts the resulting next state s~t+1:|Predictor(st,a)\u2192s~t+1\u2208S|6|\n  |--|--|\n  The Predictor is inspired by the Orbitofrontal cortex (OFC), which plays a role in estimating and predicting task states. In particular, it has been proposed that the OFC plays a key role in encoding cognitive maps: representations of task-relevant states and their relationships to one another^23^.... -\n\n**Evaluator**. The Evaluator receives a next-state prediction s~t+1 and produces an estimate of its value *v* in the context of goal *s*~*g**o**a**l*~. This is accomplished by prompting the Evaluator (and demonstrating via a few in-context examples) to estimate the minimum number of steps required to reach the goal (or subgoal) from the current state:|Evaluator(s~t+1,sgoal)\u2192v\u2208R\u22650|7|... We found that MAP outperformed both GPT-4 ICL and CoT in each of these settings, indicating that MAP can improve the generalizability and robustness of planning in LLMs.... For ToH, the TaskDecomposer generated only a single subgoal by default, but we also evaluated a version of the model in which the TaskDecomposer generated two subgoals. For the graph traversal tasks, we didn\u2019t use a separate Predictor, since the action proposed by the Actor directly specifies the next state.... The values are backpropagated to their parent states in the first layer, and the action that leads to the most valuable state is selected. In our implementation, we accelerate this process by caching the actions and predicted states from deeper search layers and then reusing them in subsequent searches. We also employ the Orchestrator to prematurely terminate the search if the goal state is achieved.",
          "doi": "10.1038/s41467-025-63804-5",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12485071/",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.2,
          "support": 0.1,
          "authority": 0.4,
          "final": 0.18,
          "evidence_snippet": "Formal analysis and verification of TPM functionalities... using ProVerif to approximate internal state space.",
          "why": "This paper is about formal verification in trusted platform modules, unrelated to AI agent architectures or programming languages like Python or Rust."
        },
        {
          "paper_id": null,
          "title": "Agentic AI: A Comprehensive Survey of Architectures ...",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "This review is structured around this framework to synthesize three critically interconnected layers:\n\nThe first layer encompasses the Theoretical Foundations, including core principles of autonomy and agency [38], and decision-making models like Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs) [39, 40]. It is crucial to note that these models provide a theoretical language for describing agency that originated in the Symbolic paradigm, but modern systems implement these concepts in entirely new ways.... #### 2.2.3 Cognitive Architectures: BDI and SOAR\n\nCognitive architectures like Belief-Desire-Intention (BDI) and SOAR represent the pinnacle of the symbolic paradigm\u2019s attempt to engineer agency. They explicitly model internal states and processes, as summarized in Table 2. These systems directly implement a perceive-plan-act-reflect loop using symbolic representations, making them powerful but brittle and difficult to scale to complex, real-world environments. Their relationship to human cognitive functions is a direct, top-down mapping of symbolic logic.... These implementations demonstrate that the paradigm choice is not merely technical but is decisively shaped by domain-specific needs, validating the need for a clear taxonomic framework to classify and select appropriate architectures.... Within the Symbolic Paradigm, coordination is achieved through pre-defined, algorithmic protocols rooted in decades of distributed AI research. These protocols are engineered to ensure predictable, verifiable, and fault-tolerant interactions, making them indispensable for critical systems where correctness is paramount. A quintessential example is the Contract Net Protocol (CNP) [91], a classic negotiation framework where a manager agent announces a task through a \u201ccall for proposals.\u201d... This analysis confirms that the paradigm shift extends to the very fabric of multi-agent coordination. The symbolic paradigm offers verifiable reliability through rigorously engineered protocols, while the neural paradigm offers adaptable emergence through learned conversation patterns. This critical distinction is essential for understanding the capabilities, risks, and appropriate applications of modern MAS, thereby further validating the necessity of the dual-paradigm framework presented in this survey.... ### 4.7 Summary of Insights\n\nSynthesizing the literature through our dual-paradigm framework reveals several fundamental distinctions and clear trajectories for the field of Agentic AI. The analysis demonstrates that paradigm divergence is fundamental; rather than representing evolutionary stages, the symbolic and neural lineages constitute parallel development paths characterized by fundamentally different operational mechanics\u2014algorithmic reasoning versus stochastic orchestration. This architectural divergence emerges as the most critical factor in determining any agentic system\u2019s inherent capabilities and limitations.",
          "doi": null,
          "url": "https://arxiv.org/html/2510.25445v1",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.2,
          "support": 0.1,
          "authority": 0.4,
          "final": 0.18,
          "evidence_snippet": "Formal analysis and verification of TPM functionalities... using ProVerif to approximate internal state space.",
          "why": "This paper is about formal verification in trusted platform modules, unrelated to AI agent architectures or programming languages like Python or Rust."
        }
      ],
      "status": "NEED_MANUAL",
      "notes": "No candidates met thresholds. Provide manual review."
    }
  ]
}
{
  "anchor_summary": {
    "topic": "Formal verification and control mechanisms for AI agent orchestration",
    "subareas": [
      "AI agent architectures",
      "Formal methods in software verification",
      "Runtime policy enforcement",
      "Human-in-the-loop and hybrid AI systems",
      "Security and access control in AI systems"
    ],
    "key_terms": [
      "Symbolic Governor",
      "System 2 cognition",
      "Agent Constitution",
      "Scheduler Interception",
      "Probabilistic CPU",
      "Policy Engine",
      "Reference Monitor",
      "Resource Limits",
      "Taint Checks",
      "Access Control Lists (ACLs)",
      "Deterministic code",
      "Formal verification"
    ],
    "likely_venues": [
      "ACM Conference on Autonomous Agents and Multiagent Systems (AAMAS)",
      "International Conference on Autonomous Agents and Multiagent Systems",
      "IEEE Symposium on Security and Privacy",
      "USENIX Security Symposium",
      "International Conference on Software Engineering (ICSE)",
      "International Joint Conference on Artificial Intelligence (IJCAI)",
      "AAAI Conference on Artificial Intelligence"
    ],
    "exclusions": [
      "Purely probabilistic or black-box AI models without formal guarantees",
      "General natural language processing without agent control focus",
      "Hardware-level CPU design unrelated to AI agent orchestration",
      "Non-deterministic runtime environments without policy enforcement"
    ]
  },
  "existing_citations_count": 0,
  "seed_expansion_used": false,
  "bib_path": "cases\\references.bib",
  "existing_entries_count": 0,
  "new_entries_added_count": 6,
  "new_bibkeys_added": [
    "UnknownIntents",
    "UnknownThe",
    "UnknownAgent",
    "UnknownVerification",
    "UnknownFormal",
    "UnknownAgenta"
  ],
  "warnings": [],
  "claims": [
    {
      "sid": "S0",
      "sentence": "The Symbolic Governor acts as the \"System 2\" component of the agent.",
      "rationale": "The sentence defines the role of the Symbolic Governor as the 'System 2' component, which is a specific conceptual claim requiring citation to prior work or authoritative source.",
      "claim_type": "definition",
      "queries": [
        "Symbolic Governor as System 2 cognition in AI agent architectures",
        "Formal verification of Symbolic Governor in agent orchestration",
        "Role of Symbolic Governor in runtime policy enforcement for AI agents",
        "Agent Constitution and Scheduler Interception with Symbolic Governor",
        "Policy Engine and Reference Monitor integration with Symbolic Governor",
        "Formal methods for System 2 components in hybrid AI systems"
      ],
      "selected": [],
      "status": "NEED_MANUAL",
      "notes": "No reliable BibTeX (missing DOI)."
    },
    {
      "sid": "S1",
      "sentence": "It is the root of trust.",
      "rationale": "The phrase 'root of trust' is a technical term that typically requires citation to clarify its specific meaning and context.",
      "claim_type": "definition",
      "queries": [
        "Symbolic Governor as root of trust in AI agent orchestration",
        "Formal verification of Policy Engine and Reference Monitor in AI systems",
        "Scheduler Interception and access control lists for secure AI agent architectures",
        "Runtime policy enforcement with Symbolic Governor and System 2 cognition",
        "Formal methods for ensuring root of trust in AI agent control mechanisms",
        "Resource Limits and Taint Checks in formally verified AI agent orchestration"
      ],
      "selected": [],
      "status": "NEED_MANUAL",
      "notes": "No reliable BibTeX (missing DOI)."
    },
    {
      "sid": "S2",
      "sentence": "Unlike the LLM, the Governor is implemented in deterministic code (e.",
      "rationale": "The claim about the Governor being implemented in deterministic code is a specific technical detail that requires verification.",
      "claim_type": "method_description",
      "queries": [
        "Symbolic Governor deterministic code formal verification",
        "Formal verification of AI agent orchestration with Symbolic Governor",
        "Deterministic code in AI agent control mechanisms",
        "Policy Engine and Reference Monitor in AI agent orchestration",
        "Scheduler Interception and runtime policy enforcement in AI systems",
        "Formal methods for resource limits and access control in AI agents"
      ],
      "selected": [],
      "status": "NEED_MANUAL",
      "notes": "No reliable BibTeX (missing DOI)."
    },
    {
      "sid": "S3",
      "sentence": "g., Python, Rust) and is formally verifiable.",
      "rationale": "The claim that the language (Python, Rust) is formally verifiable is a factual claim that requires citation to support formal verification status.",
      "claim_type": "prior_work",
      "queries": [
        "Formal verification of AI agent orchestration in Python and Rust",
        "Symbolic Governor and formal methods for AI agent control",
        "Runtime policy enforcement and Scheduler Interception in AI systems",
        "Formal verification of Policy Engine and Reference Monitor in AI agents",
        "Access Control Lists and Taint Checks for secure AI agent orchestration",
        "Deterministic code and System 2 cognition in formally verifiable AI architectures"
      ],
      "selected": [],
      "status": "NEED_MANUAL",
      "notes": "No reliable BibTeX (missing DOI)."
    },
    {
      "sid": "S4",
      "sentence": "It is responsible for the high-level orchestration of the agent's workflow and the strict enforcement of the Agent Constitution.",
      "rationale": "The sentence describes a specific responsibility and enforcement mechanism of the Agent Constitution, which is a technical claim requiring citation to prior work or formal specification.",
      "claim_type": "method_description",
      "queries": [
        "Agent Constitution enforcement in AI agent orchestration",
        "Formal verification of high-level AI agent workflow control",
        "Symbolic Governor for strict policy enforcement in AI agents",
        "Scheduler Interception and Policy Engine in agent orchestration",
        "Reference Monitor role in AI agent workflow management",
        "Formal methods for runtime policy enforcement in multiagent systems"
      ],
      "selected": [],
      "status": "NEED_MANUAL",
      "notes": "No reliable BibTeX (missing DOI)."
    },
    {
      "sid": "S5",
      "sentence": "The Governor implements a Scheduler Interception mechanism.",
      "rationale": "The sentence describes a specific mechanism implemented by the Governor, which is a technical claim requiring citation to support the method's existence or details.",
      "claim_type": "method_description",
      "queries": [
        "Governor Scheduler Interception formal verification AI agent orchestration",
        "Symbolic Governor Scheduler Interception policy enforcement AI systems",
        "Formal methods Scheduler Interception in AI agent architectures",
        "Runtime policy enforcement Scheduler Interception Governor AI",
        "Scheduler Interception and Reference Monitor in AI agent control",
        "Formal verification of Scheduler Interception mechanisms in AI agents"
      ],
      "selected": [],
      "status": "NEED_MANUAL",
      "notes": "No reliable BibTeX (missing DOI)."
    },
    {
      "sid": "S7",
      "sentence": "In ArbiterOS, the LLM emits an intent to call a tool.",
      "rationale": "The sentence describes a specific mechanism in ArbiterOS involving LLM intent emission, which is a technical detail requiring citation to support the claim.",
      "claim_type": "method_description",
      "queries": [
        "ArbiterOS LLM intent tool call formal verification",
        "Symbolic Governor tool invocation in AI agent orchestration",
        "Formal methods for intent emission in AI agent systems",
        "Policy Engine and Scheduler Interception for tool calls in ArbiterOS",
        "Runtime policy enforcement for LLM tool invocation",
        "Reference Monitor and access control in AI agent tool calls"
      ],
      "selected": [
        {
          "paper_id": null,
          "title": "Intents",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "## Add Intents and Utterances\n\n### Utterances overview\n\nUtterances are specific phrases or queries users use to express intentions. By mapping utterances to corresponding intents, the AI-agent can accurately understand and respond to user queries.\n\nYou can add intents (the purpose of the AI-agent users' responses) and utterances (the various ways users might phrase a specific intent, such as 'flight booking' or 'book flights' for the intent of booking a flight). You need to train the AI-agent to identify and understand these sentences to ensure accurate responses.... ### Best practices for creating intents and uttrances\n\nTo ensure accuracy and effectiveness of your AI-agent, adhere to these best practices while creating intents and utterances:\n\n- To effectively train the intents, ensure you create at least two distinct intents.\n\n- We recommend adding at least 15 to 20 diverse utterances for each intent.\n\n- Avoid using similar utterances both within a single flow and across multiple flows.\n\n- Ensure that all utterances in flows are unique to maintain clarity and accuracy.... ## Trigger flows using intents\n\nTo trigger a flow based on a specific intent, follow these steps:\n\n\n\nNavigate to the respective flow and click on the Start node.\n\n\n\nChoose\n\n**Intent**as the trigger type.\n\n\n\nSelect the desired intent from the drop-down.... Click\n\n**Test your bot**, enter the name of the intent that you have created, then click on **Send**icon.\n\n- This will generate code to display the AI-agent's response to that intent.\n\nIn the example code below, the AI-agent confidently recognizes the phrase as part of the\n\n**order intent**, with a confidence score of 0.999.\n\n`{`\n\n\"text\": \"place order.\",\n\n\"intents\": {\n\n\"order\": 0.999\n\n},\n\n\"global_model\": {},\n\n\"intent\": \"order\",\n\n\"confidence\": 0.999,\n\n\"global_entities\": [],\n\n\"entities\": {}... ## Stop or exit AI-agent conversations using intents\n\nYou can configure your AI-agent to stop or exit conversations using specific intents and utterances. Follow these steps to set up this functionality:\n\n\n\n- Create intents such as \"stop the flow\" or \"exit the flow\".\n\n- Add corresponding utterances that users might use to indicate they want to end the conversation.\n\n- Train the AI-agent on these intents and utterances.\n\n\n\n- Go to the flow creation section.\n\n- Create a new flow and set the previously created intent as the start trigger.",
          "doi": "10.1007/978-1-4842-6914-5_4",
          "url": "https://docs.yellow.ai/docs/platform_concepts/studio/train/intents",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.4,
          "support": 0.2,
          "authority": 0.5,
          "final": 0.315,
          "evidence_snippet": "The LLM proposes an action or plan... The system simulates the effect as a state transition... The transition is committed only if valid.",
          "why": "Discusses formal verification of agent actions and tool invocations, indicating LLMs propose actions, but does not mention ArbiterOS or explicit intent emission."
        },
        {
          "paper_id": null,
          "title": "The Reasons that Agents Act: Intention and Instrumental Goals",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "operationalise the intention with which an agent acts, relating to\nthe reasons it chooses its decision. We introduce a formal defini-\ntion of intention in structural causal influence models, grounded\nin the philosophy literature on intent and applicable to real-world\nmachine learning systems. Through a number of examples and\nresults, we show that our definition captures the intuitive notion of... intent and satisfies desiderata set-out by past work. In addition, we\nshow how our definition relates to past concepts, including actual\ncausality, and the notion of instrumental goals, which is a core idea\nin the literature on safe AI agents. Finally, we demonstrate how\nour definition can be used to infer the intentions of reinforcement... \u201cknows\", and \u201cintends\". By offering a behaviourally testable defi-\nnition of intention, we get around these problems, allowing the\nintentions of artificial systems to be characterised with precision\nusing intuitively understandable language.\nWe formalise the intention with which an agent acts, as when I\nwrite with the intention of finishing this paper [36]. This conception... of intent relates to the reasons that an agent chooses its decision,\nand importantly captures instrumental goals, which are a key notion\nin the literature on safe AI agents [4, 13, 26]. Informally, an agent\nintends to cause an outcome with its action, if guaranteeing that\nanother action would cause the outcome would make that action... evaluating an agent\u2019s intentions. A number of examples from the\nliterature on philosophy and AI demonstrate that our operational-\nisation captures the common-sense notion of intent and satisfies\nseveral desiderata for a definition of algorithmic intent set out by\nAshton [2]. Then, after discussing the background on SCIMs in... soundness and completeness results for graphical criteria of inten-\ntion in SCIMs, which are identical to the criteria for an ICI [13]. This\nis a key result which shows that our notion of intention corresponds\nto instrumental goals, which have been widely discussed in the lit-\nerature on safe AI [4, 27]. Next, in Section 6, we provide a purely... in the appendix of the arxiv version of this paper [? ].\n2\nOPERATIONALISING INTENTION\nIn this section we operationalise intention in three steps of increas-\ning refinement. Several examples demonstrate that our operational-\nisations capture the philosophical and common-sense concept and\nsatisfy desiderata for a definition of intent suitable for algorithms",
          "doi": "10.65109/vzce5163",
          "url": "http://arxiv.org/pdf/2402.07221.pdf",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.4,
          "support": 0.2,
          "authority": 0.5,
          "final": 0.315,
          "evidence_snippet": "The LLM proposes an action or plan... The system simulates the effect as a state transition... The transition is committed only if valid.",
          "why": "Discusses formal verification of agent actions and tool invocations, indicating LLMs propose actions, but does not mention ArbiterOS or explicit intent emission."
        },
        {
          "paper_id": null,
          "title": "Agent-Based Explanations in AI: Towards an Abstract Framework",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "In this paper, we propose an abstract framework for XAI relying on notions and results from the MAS literature. The framework is mostly targetting sub-symbolic AI and ML-based intelligent systems. In particular, our framework introduces a clear distinction among two orthogonal, yet interrelated, activities \u2013 i.e., *interpretation* and *explanation* \u2013 which can be performed on sub-symbolic predictors to make them more understandable in the eyes of human beings. Thus, it provides a formal definition for such activities in the MAS perspective, thus stressing the *objective* nature of explanation, other than the *subjective* nature of interpretation.... According to this classification, while interpretable systems can be inspected to be understood \u2013 thus letting observer draw their explanations by themselves\u2013comprehensible systems must explicitly provide a symbolic explanation of their functioning. The focus is thus on *who* produces explanations, rather than *how*.... ## Explanation vs. Interpretation\n\nThis section introduces the preliminary notions, intuitions, and notations we leverage upon in Sect.\u00a03.1 and subsequent sections, in order to formalise our abstract framework for agent-based explanations. We start by providing an intuition for the notion of *interpretation*, and, consequently, for the *act* of interpreting something. Accordingly, we provide an intuition for the property of \u201cbeing interpretable\u201d as well, stressing its comparative nature. Analogously to what we did with *interpretation*, we then provide intuitions for terms such as *explanation* and its derivatives.... Summarising, we stress the subjective nature of interpretations, as agents assign them to objects according to their State of Mind (SoM) [19] and background knowledge, and they need not be formally defined any further.... Furthermore, we define an explanation M\u2032 as *admissible* if it has a valid fidelity w.r.t. the original model *M* and the data in *Z*\u2014where *Z* is the same subset of the input data used by the explanation strategy. More precisely, we say M\u2032 is \u03b4-admissible in *Z* w.r.t. *M* if \u0394f(M(Z),M\u2032(Z))<\u03b4.... Furthermore, our framework explicitly recognises the *subjective* nature of interpretation, as well as the subtly *objective* nature of explanation. Indeed, interpretation is a subjective activity directly related to agents\u2019 perception and SoM, whereas explanation is an epistemic, computational action which aims at producing a high-fidelity model.",
          "doi": "10.1007/978-3-030-51924-7_1",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7338184/",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.4,
          "support": 0.2,
          "authority": 0.5,
          "final": 0.315,
          "evidence_snippet": "The LLM proposes an action or plan... The system simulates the effect as a state transition... The transition is committed only if valid.",
          "why": "Discusses formal verification of agent actions and tool invocations, indicating LLMs propose actions, but does not mention ArbiterOS or explicit intent emission."
        }
      ],
      "status": "NEED_MANUAL",
      "notes": "No candidates met thresholds. Provide manual review."
    },
    {
      "sid": "S8",
      "sentence": "The Governor intercepts this intent, suspends the Probabilistic CPU, and evaluates the intent against the active Policy Engine.",
      "rationale": "Describes a specific mechanism involving the Governor, Probabilistic CPU, and Policy Engine which likely requires citation to prior work or formal definition.",
      "claim_type": "method_description",
      "queries": [
        "Symbolic Governor intercepting intent in AI agent orchestration",
        "Probabilistic CPU suspension and Policy Engine evaluation in AI systems",
        "Formal verification of Policy Engine and Scheduler Interception mechanisms",
        "Runtime policy enforcement using Reference Monitor and Symbolic Governor",
        "Agent Constitution and System 2 cognition in formal AI control architectures",
        "Access Control Lists and Taint Checks for secure AI agent orchestration"
      ],
      "selected": [
        {
          "paper_id": null,
          "title": "Verification and Enforcement of Access Control Policies",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "tem insecure. The security of the system, however, does not only depend on the correct\nspecification of the security policy, but in a large part also on the correct interpretation of\nthose rules by the system\u2019s enforcement mechanism.\nIn this paper, we show how policy rules can be formalized in Fusion Logic, a temporal... logic for the specification of behavior of systems. A symbolic decision procedure for Fusion\nLogic based on Binary Decision Diagrams (BDDs) is provided and we introduce a novel\ntechnique for the construction of enforcement mechanisms of access control policy rules\nthat uses a BDD encoded enforcement automaton based on input traces which reflect state... changes in the system. We provide examples of verification of policy rules, such as absence\nof conflicts, and dynamic separation of duty and of the enforcement of policies using our\nprototype implementation (FLCheck) for which we detail the underlying theory.\nKeywords Access control policy \u00b7 Policy Enforcement \u00b7 Policy Verification \u00b7 Binary\nDecision Diagram... verify properties of the policy specification, e.g., accessibility, dynamic separation of\nduty, dynamic and static conflicts.\n\u2013 We describe a decision procedure for Fusion Logic we have developed that uses Binary\nDecision Diagrams (BDDs), and show how this decision procedure can be adapted to\nact as an enforcement mechanism for history-based access control policies.... The framework consists of three components: Specification, Verification and Enforce-\nment. In the Specification component, one specifies a system by a set of history-based access\ncontrol rules. In the Verification component, one takes this set of rules together with prop-\nerties that need to hold and check whether this set of rules satisfy these properties. In the... tained. The system model is the starting point for verification and enforcement. In Section 4\nwe present an automatic way of checking the satisfiability and validity of Fusion Logic for-\nmulae. We also describe how one can derive enforcement mechanisms for history-based\naccess control policies, expressed in Fusion Logic, based on the same techniques that are... used to determine the satisfiability/validity of Fusion Logic formulae. In Section 5 we il-\nlustrate the verification of properties and the derivation of an enforcement mechanism for a\npolicy-based system with the help of case studies. We conclude our paper in Section 6.",
          "doi": "10.1007/s10703-013-0187-3",
          "url": "http://antonio-cau.co.uk/ITL/publications/pubs/2013-10.pdf",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.5,
          "support": 0.4,
          "authority": 0.5,
          "final": 0.45,
          "evidence_snippet": "The process reference monitor is a common technique to enforce security policies for application execution... The reference requests are made in the form of remote procedure calls to the monitor... Based on a pre-defined policy, the monitor may enforce an access control model",
          "why": "This paper discusses a reference monitor enforcing security policies and intercepting requests, which aligns with the concept of a Governor intercepting intents and evaluating them against policies."
        }
      ],
      "status": "NEED_MANUAL",
      "notes": "No candidates met thresholds. Provide manual review."
    },
    {
      "sid": "S10",
      "sentence": "g., Resource Limits, Taint Checks, ACLs) does the Governor execute the tool.",
      "rationale": "The sentence describes a specific method or mechanism involving Resource Limits, Taint Checks, and ACLs in the Governor's execution, which requires citation to support the technical claim.",
      "claim_type": "method_description",
      "queries": [
        "Formal verification of Symbolic Governor in AI agent orchestration",
        "Runtime policy enforcement using Resource Limits and Taint Checks in AI systems",
        "Access Control Lists and Reference Monitor for secure AI agent architectures",
        "Scheduler Interception and Policy Engine for deterministic code execution in AI agents",
        "Formal methods for hybrid AI systems with human-in-the-loop control",
        "Security and access control mechanisms in multiagent systems with formal guarantees"
      ],
      "selected": [
        {
          "paper_id": null,
          "title": "Formal Verification of Open Multi-Agent Systems",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "These have taken the form of verification techniques based both\non model checking [16, 18, 25] and theorem proving [1, 31]. A focus\nof attention has been the development of verification methods\nthat support agent-based specifications, including the temporal\nevolution of the knowledge of the agents, their desires, intentions,... for OMAS and to the practical challenges of verifying OMAS.\nThe rest of the paper is organised as follows. After discussing\nrelated work, in Section 2 we propose a semantics for OMAS, show\nhow expressive temporal-epistemic specifications can be evaluated\non them, define the verification problem and show its undecidability.... In Sections 3 and 4 we identify classes of synchronous and asyn-\nchronous OMAS and present decision procedures that address the\nverification problem against the specifications defined in Section 2.\nIn Section 5 we present an open-source toolkit implementing the\nprocedures of Sections 3 and 4, and apply it to scenarios from MAS.... Act, P,t\u27e9consisting of a set of local states L, a unique initial state\n\u03b9 \u2208L, a non-empty set of actions Act, a protocol P : L \u2192P(Act)\nthat selects which actions may be performed at a given state, and a\ntransition function t : L \u00d7 Act \u00d7 P(Act) \u00d7 ActE \u2192L that gives the... states LE, a unique initial state \u03b9E \u2208LE, a non-empty set of actions\nActE, a protocol PE : LE \u2192P(ActE) that defines which actions\nare enabled at each local state, and a transition function tE : LE \u00d7\nActE \u00d7 P(Act) \u2192LE that gives the environment\u2019s next state given... agents\u2019 local states.\nHereafter we associate with each agent an integer giving it an\nidentity. The identity of an agent is uniquely assigned to it when it\nfirst joins the system, and never changes for as long as the agent\nremains in the system. This will allow us to express properties that\ntrack the evolution of a specific agent.... i-th agent in \u0434, si(\u0434) to represent the local state of the agent with\nidentity i in \u0434, and \u0434.E to represent the state of the environment in\n\u0434. We use G \u225c\u222an\u2208NGn for the set of all global states of any size.\nSimilarly, for a joint action a, we use a.i to denote the action of",
          "doi": "10.65109/shtg3566",
          "url": "https://aamas.csc.liv.ac.uk/Proceedings/aamas2019/pdfs/p179.pdf",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.6,
          "support": 0.5,
          "authority": 0.5,
          "final": 0.5349999999999999,
          "evidence_snippet": "Provability Fabric enforces provable behavioral guarantees through formal verification, runtime security mechanisms, and audit trails.",
          "why": "This framework enforces behavioral guarantees and security at runtime, which aligns with the concept of a Governor executing tools with constraints like resource limits and ACLs."
        }
      ],
      "status": "NEED_MANUAL",
      "notes": "No candidates met thresholds. Provide manual review."
    },
    {
      "sid": "S11",
      "sentence": "This \"Reference Monitor\" pattern  ensures that the agent's cognitive instability cannot propagate to the external world.",
      "rationale": "The statement attributes a specific security property to the 'Reference Monitor' pattern, which is a technical claim requiring support from prior work or formal verification literature.",
      "claim_type": "prior_work",
      "queries": [
        "Reference Monitor pattern formal verification AI agent cognitive stability",
        "Symbolic Governor and Reference Monitor in AI agent orchestration",
        "Formal methods for preventing cognitive instability propagation in AI agents",
        "Policy Engine and Reference Monitor for runtime enforcement in AI systems",
        "Agent Constitution and Scheduler Interception for secure AI agent control",
        "Access Control Lists and Taint Checks in AI agent formal verification"
      ],
      "selected": [
        {
          "paper_id": null,
          "title": "Agent-Based Explanations in AI: Towards an Abstract Framework",
          "authors": [],
          "year": null,
          "venue": null,
          "abstract": "The opaqueness of ML-based solutions is an unacceptable condition in a world where ML is involved in many (safety-)critical activities. Indeed, performing automatic, good predictions (resp. provide useful decisions) is essential as much as letting the humans involved in those contexts *understand* why and how such predictions (resp. decisions) have been obtained.... In this paper, we propose an abstract framework for XAI relying on notions and results from the MAS literature. The framework is mostly targetting sub-symbolic AI and ML-based intelligent systems. In particular, our framework introduces a clear distinction among two orthogonal, yet interrelated, activities \u2013 i.e., *interpretation* and *explanation* \u2013 which can be performed on sub-symbolic predictors to make them more understandable in the eyes of human beings. Thus, it provides a formal definition for such activities in the MAS perspective, thus stressing the *objective* nature of explanation, other than the *subjective* nature of interpretation.... Clearly, troubles in understanding black-box content and functioning prevent people from fully trusting \u2013 therefore accepting \u2013 them. To make the picture even more complex, current regulations such as the GDPR [25] are starting to recognise the citizens\u2019 *right to explanation* [12]\u2014which implicitly requires IS to eventually become *understandable*. Indeed, understanding IS is essential to guarantee algorithmic fairness, to identify potential bias/problems in the training data, and to ensure that IS perform as designed and expected.... As far as explainability is concerned, apparently, Rudin adopts a *post-hoc* perspective similar to the one in [15], as she writes, \u201can explanation is a separate model that is supposed to replicate most of the behaviour of a black box\u201d. In the remainder of that paper, the author argues how the path towards interpretable ML steps through broader adoption of inherently interpretable predictors \u2013 such as generalised linear models or decision trees \u2013 rather than relying on *post-hoc* explanations which do not reveal what is inside black boxes\u2014thus preventing their full understanding.... Conversely, it may also happen the explanation *E* adopted by *A* produces an object X\u2032, which is more interpretable than *X* for *A* but not for *B*. Similarly to how two persons would handle such an unpleasant situation, we envision that interaction and communication may be adopted to break such *impasses* in multi-agent systems.",
          "doi": "10.1007/978-3-030-51924-7_1",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC7338184/",
          "citation_count": null,
          "source": "perplexity",
          "seed_boost": 0.0,
          "relevance": 0.6,
          "support": 0.5,
          "authority": 0.6,
          "final": 0.5499999999999999,
          "evidence_snippet": "Policy Cards enable agents to follow required constraints at runtime... enabling verifiable compliance for autonomous agents.",
          "why": "Policy Cards provide runtime governance mechanisms that can enforce constraints preventing undesired propagation of agent instability."
        }
      ],
      "status": "NEED_MANUAL",
      "notes": "No candidates met thresholds. Provide manual review."
    }
  ]
}